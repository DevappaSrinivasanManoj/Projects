{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iKaKcumhLgp4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TinyTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_output)  # Add & Norm\n",
        "\n",
        "        # Feedforward\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + ff_output)  # Add & Norm\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GPTMini(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TinyTransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.block_size, \"Input sequence too long\"\n",
        "\n",
        "        # Token + positional embeddings\n",
        "        tok_emb = self.token_embedding(idx)  # (B, T, embed_dim)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, embed_dim)\n",
        "        x = tok_emb + pos_emb  # (B, T, embed_dim)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "33wgC14pLlmw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy dataset\n",
        "text = \"hello world\"\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mapping from char to index and vice versa\n",
        "stoi = { ch: i for i, ch in enumerate(chars) }\n",
        "itos = { i: ch for ch, i in stoi.items() }\n",
        "\n",
        "# Encode/decode helpers\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Prepare dataset\n",
        "block_size = 8  # context length\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "def get_batch():\n",
        "    i = torch.randint(len(data) - block_size, (1,))\n",
        "    x = data[i:i+block_size].unsqueeze(0)      # shape: (1, block_size)\n",
        "    y = data[i+1:i+block_size+1].unsqueeze(0)  # next tokens\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tD9o9r0dLs_8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTMini(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=block_size,\n",
        "    embed_dim=32,\n",
        "    num_heads=4,\n",
        "    ff_hidden_dim=64,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(1000):\n",
        "    x_batch, y_batch = get_batch()\n",
        "    logits = model(x_batch)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}: loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erVRrgmzLvPj",
        "outputId": "2d522553-7539-47e9-e453-0d9abae29d56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: loss = 2.2375\n",
            "Step 100: loss = 0.0881\n",
            "Step 200: loss = 0.0323\n",
            "Step 300: loss = 0.0170\n",
            "Step 400: loss = 0.0111\n",
            "Step 500: loss = 0.0077\n",
            "Step 600: loss = 0.0056\n",
            "Step 700: loss = 0.0031\n",
            "Step 800: loss = 0.0033\n",
            "Step 900: loss = 0.0027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, start_text, length):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([encode(start_text)], dtype=torch.long)\n",
        "    for _ in range(length):\n",
        "        idx_crop = idx[:, -block_size:]  # crop context\n",
        "        logits = model(idx_crop)\n",
        "        logits = logits[:, -1, :]  # last token logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "    return decode(idx[0].tolist())"
      ],
      "metadata": {
        "id": "bW7ABob0L0Ly"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, start_text=\"h\", length=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5tlkX9oL2lb",
        "outputId": "ce371a79-4a2b-4649-a699-c10cfb678752"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello \n"
          ]
        }
      ]
    }
  ]
}